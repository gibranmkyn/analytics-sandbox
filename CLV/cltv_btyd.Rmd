---
title: "btyd_model"
author: "Gibran Makyanie"
date: "18/03/2020"
output: html_document
---

```{r setup, include=FALSE}
rm(list = ls())
library(tidyverse)
library(lubridate)
library(BTYD)
library(plyr)
```


“Buy ‘Til You Die” probabilistic models help us in quantifying the lifetime value of a customer by assessing the expected number of his future transactions and his probability of being “alive”.

Data are 10% of the cohort of customers who made their first transactions with online retailer CDNOW (founded 1994) in the first quarter of 1997. 

```{r}
cdnowElog <- system.file("data/cdnowElog.csv", package = "BTYD")

dataset <- read.csv(cdnowElog)

dataset <- dataset %>%
 select(sampleid, date, sales) %>%
 mutate(date = as.Date(as.character(date), format="%Y%m%d")) %>%
 mutate(cust = sampleid) %>%
 select(-sampleid)

  
```


Transaction-flow models, such as the Pareto/NBD, are concerned with interpurchase intervals. Since we only have dates and there may be multiple purchases on a day we merge all transactions that occurred on the same day using dc.MergeTransactionsOnSameDate()


To validate that the model works, we need to divide the data up into a calibration period and a holdout period. This is relatively simple with either an event log or a customer-by-time matrix, which we are going to create soon. I am going to use 30 September 1997 as the cutoff date, as this point (39 weeks) divides the dataset in half. The reason for doing this split now will become evident when we are building a customer-by-sufficient-statistic matrix from the customer-by-time matrix—it requires a last transaction date, and we want to make sure that last transaction date is the last date in the calibration period and not in the total period.
# Pareto/NBD
```{r}
final_df <- dc.MergeTransactionsOnSameDate(dataset) # get sum transactions per customer per day
summary(final_df)



end.of.cal.period <- as.Date("1997-09-30")
dataset.cal <- dataset[which(dataset$date <= end.of.cal.period), ] # splits by 30th Sep 1997
```


The final cleanup step is a very important one. In the calibration period, the Pareto/NBD model is generally concerned with repeat transactions—that is, the first transaction is ignored.

The one problem with simply getting rid of customers’ first transactions is the following: We have to keep track of a
“time zero” as a point of reference for recency and total time observed. For this reason, we use dc.SplitUpElogForRepeatTrans, which returns a filtered event log ($repeat.trans.elog) as well as saving important information about each customer ($cust.data).


```{r}
split.data <- dc.SplitUpElogForRepeatTrans(dataset.cal) 
clean.dataset <- split.data$repeat.trans.elog
```

The next step is to create a customer-by-time matrix. This is simply a matrix with a row for each customer and a column for each date. There are several different options for creating these matrices:
- Frequency—each matrix entry will contain the number of transactions made by that customer on that day. Use dc.CreateFreqCBT. If you have already used dc.MergeTransactionsOnSameDate, this will simply be a reach customer-by-time matrix.
- Reach—each matrix entry will contain a 1 if the customer made any transactions on that day, and 0 otherwise. Use dc.CreateReachCBT.
- Spend—each matrix entry will contain the amount spent by that customer on that day. Use dc.CreateSpendCBT. You can set whether to use to- tal spend for each day or average spend for each day by changing the is.avg.spend parameter. In most cases, leaving is.avg.spend as FALSE is appropriate.

```{r}

#customer by time
freq.cbt <- dc.CreateFreqCBT(clean.dataset)
tot.cbt <- dc.CreateFreqCBT(dataset)
cal.cbt <- dc.MergeCustomers(tot.cbt, freq.cbt)


birth.periods <- split.data$cust.data$birth.per 
last.dates <- split.data$cust.data$last.date 
cal.cbs.dates <- data.frame(birth.periods, last.dates,end.of.cal.period)
cal.cbs <- dc.BuildCBSFromCBTAndDates(cal.cbt, cal.cbs.dates,per="week")

head(cal.cbs)
```

The function which is maximized is pnbd.cbs.LL, which returns the log-likelihood of a given set of parameters for a customer-by-sufficient-statistic matrix.

```{r}
params <- pnbd.EstimateParameters(cal.cbs); 
head(params)

LL <- pnbd.cbs.LL(params, cal.cbs)  # log likelihood
LL


p.matrix <- c(params, LL)
for (i in 1:2){
params <- pnbd.EstimateParameters(cal.cbs, params)
LL <- pnbd.cbs.LL(params, cal.cbs)
p.matrix.row <- c(params, LL)
p.matrix <- rbind(p.matrix, p.matrix.row)
}
colnames(p.matrix) <- c("r", "alpha", "s", "beta", "LL"); rownames(p.matrix) <- 1:3
p.matrix
```


```{r}
pnbd.PlotTransactionRateHeterogeneity(params)
```
We can see this gamma distribution in figure. Customers more likely to have low values for their individual poisson transaction process parameters, 

```{r}
pnbd.PlotDropoutRateHeterogeneity(params)
```
We also know that s and beta describe the gamma mixing distribution of the Pareto (or gamma exponential) dropout process. Customers are more likely to have low values for their individual exponential dropout process parameters.


# Individual Level Estimation

First, we can estimate the number of transactions we expect a newly acquired customer to make in a given time period. Let’s say, for example, that we are interested in the number of repeat transactions a newly acquired customer will make in a time period of one year. Note that we use 52 weeks to represent one year, not 12 months, 365 days, or 1 year. This is because our parameters were estimated using weekly data.

```{r}
pnbd.Expectation(params, t=52)
```

We can also obtain expected characteristics for a specific customer, conditional on their purchasing behavior during the calibration period. 

The first of these is pnbd.ConditionalExpectedTransactions, which gives the number of transactions we expect a customer to make in the holdout period. 

The second is pnbd.PAlive, which gives the probability that a customer is still alive at the end of the calibration period. As above, the time periods used depend on which time period was used to estimate the parameters.


```{r}
cal.cbs["1516",]

x <- cal.cbs["1516", "x"]
t.x <- cal.cbs["1516", "t.x"]
T.cal <- cal.cbs["1516", "T.cal"] 

pnbd.ConditionalExpectedTransactions(params, T.star = 52, x, t.x, T.cal) # [1] 25.46
pnbd.PAlive(params, x, t.x, T.cal) # [1] 0.9979

```


```{r}
 for (i in seq(10, 25, 5)){
cond.expectation <- pnbd.ConditionalExpectedTransactions(params, T.star = 52, x = i, t.x = 20, T.cal = 39)
cat ("x:",i,"\t Expectation:",cond.expectation, fill = TRUE) 
}
```


# Goodness of Fit

We would like to be able to do more than make inferences about individual customers. The BTYD package provides functions to plot expected customer behavior against actual customer behaviour in the both the calibration and holdout periods.
The first such function is the obvious starting point: a comparison of actual and expected frequencies within the calibration period. This is figure 1, which was generated using the following code:

```{r}
pnbd.PlotFrequencyInCalibration(params, cal.cbs, 7)
```

Unfortunately, the only thing we can tell from comparing calibration period frequencies is that the fit between our model and the data isn’t awful. We need to verify that the fit of the model holds into the holdout period. Firstly, however, we are are going to need to get information for holdout period.

dc.ElogToCbsCbt produces both a calibration period customer-by-sufficient-statistic matrix and a holdout period customer-by-sufficient-statistic matrix, which could be combined in order to find the number of transactions each customer made in the holdout period. 

However, since we did not use dc.ElogToCbsCbt, I am going to get the information directly from the event log. Note that I subtract the number of repeat transactions in the calibration period from the total number of transactions. We remove the initial transactions first as we are not concerned with them.

```{r}
elog <- dc.SplitUpElogForRepeatTrans(elog)$repeat.trans.elog; x.star <- rep(0, nrow(cal.cbs))
cal.cbs <- cbind(cal.cbs, x.star)
elog.custs <- elog$cust

for (i in 1:nrow(cal.cbs)){ 
  current.cust <- rownames(cal.cbs)[i]
  tot.cust.trans <- length(which(elog.custs == current.cust)) 
  cal.trans <- cal.cbs[i, "x"]
  cal.cbs[i, "x.star"] <- tot.cust.trans - cal.trans
}

cal.cbs[1:3,]



T.star <- 39 # length of the holdout period
censor <- 7 # This censor serves the same purpose described above x.star <- cal.cbs[,"x.star"]
comp <- pnbd.PlotFreqVsConditionalExpectedFrequency(params, T.star,cal.cbs, x.star, censor)
```








```{r}
ggplot(final_df, aes(x=date,y=sales,group=cust))+
    geom_line(alpha=0.1)+
    scale_x_date()+
    scale_y_log10()+
    ggtitle("Sales for individual customers")+
    ylab("Sales ($, US)")+xlab("")+
    theme_minimal()
```

```{r}
# ----- get purchase daysbetween
purchaseFreq <- ddply(final_df, .(cust), summarize, 
     daysBetween = as.numeric(diff(date)))

ggplot(purchaseFreq,aes(x=daysBetween))+
    geom_histogram(fill="orange")+
    xlab("Time between purchases (days)")+
    theme_minimal()
```

```{r}
elog <- dc.MergeTransactionsOnSameDate(elog);
```

