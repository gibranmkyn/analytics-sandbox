---
title: "Airbnb Webscrapper"
author: "Gibran Makyanie"
date: "11/02/2020"
output:
  word_document: default
  html_document: default
---

```{r}
knitr::opts_chunk$set(eval = FALSE)
```

```{r setup, include=FALSE}
rm(list = ls()) # cleans the memory
library(xml2)
library(rvest)
library(dplyr)
library(qdap) # Quantitative Discourse Analysis Package
library(ggplot2)
library(RCurl) # download files
library(readr) # read from zip
library(stringr)
library(RSQLite)
# library(plyr)
library(cld3) # language detector
library(tidyselect)
library(lubridate)
```


## Web Scrapping
The main goal of this section is to download the relevant datasets from the insideairbnb webpage. First, the URLs of the datasets along with its descriptions such as compiled date and city name are scrapped from the web using the xml2 package and compiled into a dataframe. Next, the dataframe is filtered to those relevant for the assignment stored in 'to_download' dataframe. Finally, a list of the relevant URLs are passed to a loop that downloads the data into a folder called 'dataset'.


```{r get the URLs of airbnb datasets}

split_path <- function(path) {
    rev(setdiff(strsplit(path,"/|\\\\")[[1]], ""))
} 

# ----- Read HTML of the airbnb open data
url_airbnb <- 'http://insideairbnb.com/get-the-data.html'
doc <- read_html(url_airbnb) # Assign results to `doc`  

web_list  <- doc %>% 
          html_nodes('tbody tr td') %>%
          html_text() 

# ----- Create a df in a tidy format on all datasets available including the archived ones
file_path  <- doc %>%  html_nodes('tbody tr td a') %>% html_attr('href') # file_path column
date_compiled  <- web_list[seq(1, length(web_list), 4)] # date_compiled column
country_or_city <- web_list[seq(2, length(web_list), 4)] # country_or_city column
file_name <- web_list[seq(3, length(web_list), 4)] # file_name column
description <- web_list[seq(4, length(web_list), 4)] # description column
date <- c() # get date column from file path
    for (i in 1:length(file_path)) {
    new_date <-split_path(file_path[i])[3]
    date <- c(date, new_date)
    }

airbnb_map_all <- data.frame(date_compiled, date, country_or_city, file_name, file_path, description, stringsAsFactors = FALSE) # airbnb_resource dataset
airbnb_map_all$date <- as.Date(airbnb_map_all$date)
```


```{r filter the datasets to those relevant for the assignment}
# ----- Filter only the latest datasets for each country and take only the zipped files for reviews and listings
city_newest_update <- airbnb_map_all %>%
   group_by(country_or_city) %>%
   summarise(date = max(date))

to_download_review_listing <- merge(airbnb_map_all, city_newest_update, by = c("country_or_city","date")) %>%
  filter(file_name %in% c("reviews.csv.gz", "listings.csv.gz")) %>%
  mutate(destination_file = paste0("dataset/", country_or_city, "_", file_name))

# ----- Filter only calendar data including archived datasets
to_download_calendar <- airbnb_map_all %>%
  filter(file_name == "calendar.csv.gz") %>%
  arrange(country_or_city, desc(date)) %>%
  mutate(destination_file = paste0("dataset/",country_or_city, "_", as.character(date), "_", file_name))
```


```{r download relevant files}
dirname <- "dataset" 
dir.create(dirname) # create a new directory for the dataset

# ----- download review and listing datasets
for (i in 1:length(to_download_review_listing$file_path)) {
  tryCatch(download.file(to_download_review_listing$file_path[i], destfile = destination_file[i]), error = function(e) print('file didnot work'))
}

# ----- download calendar datasets
for (i in 1:length(to_download_calendar$file_path)) {
  tryCatch(download.file(to_download_calendar$file_path[i], destfile = destination_file[i] ), error = function(e) print('file didnot work'))
}
```

## Extracting, Transforming, and Loading data to a relational schema

A complete ETL workflow has to be defined to get the relevant datasets into a normalised relational schema. This might be the most essential process for the whole assignment as for the famous phrase of data scieince, "garbage in, garbage out", we don't want that!

The ETL workflows are split into 3 major processes; investigating the datasets, building a taylored-fit ETL workflow, and running the workflow. First, samples of listings and reviews data are taken from each country to know which columns to keep and which ones to remove. It was found that the listings dataset can be normalised into reviewer and review, while listing can normalised into host and listing. 

> A reviewer writes a review for a listing, which is owned by a host.

```{r ETL investigation, message=FALSE}
# ----- Pre-ETL investigation to get all columns and select the columns needed

# Create a function to make a sample of 1 row from each dataset and combine them to get an idea what columns we have
get_sample <- function(folder_path, pattern) {

  listed_files <- list.files(folder_path, pattern = pattern)
  main_df <- data.frame()

  for (i in 1:length(listed_files)) {
    
    file_path <-paste(folder_path, listed_files[i],sep="/")
    local_df <- read.csv(file_path, nrows= 1, stringsAsFactors=FALSE)
    #local_df <- read_csv(file_path, n_max = 1)
    local_df$file_name <-listed_files[i]
    local_df$pre_processed <- 0
    main_df <- plyr::rbind.fill(main_df, local_df)
    
  }

return(main_df)

}

# Generate the sample
listings_sample <- get_sample(folder_path = "dataset", pattern = "listings.csv.gz")
reviews_sample <- get_sample(folder_path = "dataset", pattern = "reviews.csv.gz")
calendar_sample <- get_sample(folder_path = "dataset", pattern = "calendar.csv.gz")
reviews_sample$date <- as.Date(reviews_sample$date)

```

Building the ETL workflow requires the most effort of the ETL process as we need to balance between having too much data and too less of a data. After investigating the columns from get_sample function we defined earlier, we can already reduce the not-going to be used columns early. By filtering columns early we reduce the number of dimensions thus will reduce the amount of time later to process the datasets. Even delaying the decision to filter columns later will obviously result to more processing time.

Thus, we build the transformation process on the following criterions: 
* Listings and Reviews that are to be considered will be taken from the most recent datasets available in opendata Airbnb site.
* Listing should has at least 10 reviews
* Only consider reviews and calendar data of the selected listings
* Listing and reviews which are written in English
* Calendar data are taken from the archived, the most recent record of price and booking are the one to be taken

The columns which are less likely to be used such as that URLs are left out.Afterwards, the ETL work flow is built and immedietly used to the sample data as it has all the columns from every country to avoid errors later since relational schema has to be predefined before data are insereted. Ofcourse the tables are truncated to ensure clean sheet before loading the datasets (staging process).

```{r ETL build workflow}

# ----- Initiation
conn <- dbConnect(RSQLite::SQLite(), "dataset/inside_airbnb.db") # connect to SQLite db, in this case, it created a new db


# ----- Build ETL workflow for listings data and reviews data

normalise_listings <- function(listings_data) {

  remove_columns <- c('street', 'neighbourhood', 'latitude','longitude', 'is_location_exact', 'square_feet', 'license', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms','reviews_per_month', 'last_searched', 'region_id', 'region_name', 'region_parent_id', 'region_parent_name', 'region_parent_parent_id', 'region_parent_parent_name', 'weekly_price', 'monthly_price', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'has_availability', 'summary', 'description', 'neighborhood_overview', 'space', 'host_listings_count', 'smart_location', 'scrape_id', 'experiences_offered', 'notes', 'access', 'interaction', 'house_rules', 'jurisdiction_names', 'calendar_updated', 'last_review')


  # Listing Table
  listings_table <- listings_data %>%
    filter(number_of_reviews > 10) %>%
    unite(col=new_description,c(summary,description,neighborhood_overview, space),sep = " ", na.rm=TRUE) %>%
    mutate(lang = detect_language(new_description)) %>% 
    filter(lang == 'en') %>%
    dplyr::rename(listing_id = id) %>%
    mutate(listing_id = as.character(listing_id)) %>%
    select(-c(contains("url"), host_name:host_identity_verified)) %>%
    select_if(!names(.) %in% remove_columns) %>%
    mutate(price = as.numeric(gsub(",", "", substring(price, 2)))) %>%
    mutate(pre_processed = 0)
    
  # Host Table
  host_distinct <- unique(listings_table$host_id)  
  
  hosts_table <- listings_data %>%
    filter(host_id %in% host_distinct) %>%
    select(starts_with('host'), -contains("url")) %>% 
    distinct(host_id, .keep_all = TRUE)
  
  # Insert to db
  dbWriteTable(conn,"host", hosts_table, append = TRUE)
  dbWriteTable(conn,"listing", listings_table, append = TRUE)
}

normalise_reviews <- function(reviews_data, included_listing) {
  
  # Review table
  reviews_table <- reviews_data %>% 
    mutate(listing_id = as.character(listing_id)) %>%
    filter(listing_id %in% included_listing$listing_id) %>%
    mutate(lang = detect_language(comments)) %>% 
    filter(lang == 'en') %>%
    dplyr::rename(review_id = id) %>%
    mutate(review_id = as.character(review_id)) %>%
    select(-reviewer_name) %>%
    mutate(pre_processed = 0)
    
  
  # Reviewer Table
  review_distinct <- unique(reviews_table$review_id)  
  
  reviewers_table <- reviews_data %>%
    dplyr::rename(review_id = id) %>%
    mutate(review_id = as.character(review_id)) %>%
    filter(review_id %in% review_distinct) %>%
    distinct(reviewer_id, reviewer_name)
    
  dbWriteTable(conn,"review", reviews_table, append = TRUE)
  dbWriteTable(conn,"reviewer", reviewers_table, append = TRUE)
}

normalise_calendar <- function(calendars_data, included_listing) {
  
   remove_columns <- c('adjusted_price', 'minimum_nights', 'maximum_nights', 'available')
   
  # Calendar table
  calendar_table <- calendars_data %>% 
    mutate(listing_id = as.character(listing_id)) %>%
    filter(listing_id %in% included_listing$listing_id, year(date) <= 2019) %>%
    mutate(booked = ifelse(available==FALSE, 1, 0)) %>%
    mutate(price = as.numeric(gsub(",", "", substring(price, 2)))) %>%
    select_if(!names(.) %in% remove_columns) %>%
    anti_join(calendar_tracker)

    
  dbWriteTable(conn,"calendar", calendar_table, append = TRUE)
  
  calendar_tracker <- dbGetQuery(conn,"SELECT distinct listing_id, date FROM calendar") %>%
    mutate(date = as_date(date))
  
  assign("calendar_tracker", calendar_tracker, envir = .GlobalEnv)
}


# ----- Automatically use sample data to create schema
normalise_listings(listings_sample)
included_listing <- dbGetQuery(conn, 'SELECT listing_id FROM listing')
normalise_reviews(reviews_sample, included_listing)

# ----- Clear tables, ready to be inserted.
dbExecute(conn, "DELETE FROM review")
dbExecute(conn, "DELETE FROM reviewer")
dbExecute(conn, "DELETE FROM listing")
dbExecute(conn, "DELETE FROM host")

```

Finally, the datasets are loaded by running the ETL workflow with the prebuilt functions on normalising the datasets and the use of loops.

```{r ETL run workflow}
start_time <- Sys.time()

folder_path <- "dataset"
listings_list <-list.files(folder_path, pattern = "listings.csv.gz")
reviews_list <-list.files(folder_path, pattern = "reviews.csv.gz")
calendars_list <-list.files(folder_path, pattern = "calendar.csv.gz")
calendars_list <- to_download_calendar %>%
  inner_join(data.frame(destination_file = paste0('dataset/',calendars_list)))
calendars_list <- calendars_list$destination_file

# ----- Run ETL to normalise listings.csv.gz and store in a relational schema
for (i in 1:length(listings_list)) {
  
  file_path <-paste(folder_path, listings_list[i],sep="/")
  listings_data <- read_csv(file_path)
  listings_data$file_name <-listings_list[i]

  normalise_listings(listings_data) # call function built especially to normalise listings

}

# ----- Run ETL to normalise reviews.csv.gz and store in a relational schema

included_listing <- dbGetQuery(conn, 'SELECT listing_id FROM listing')

for (i in 1:length(reviews_list)) {
  
  file_path <-paste(folder_path, reviews_list[i],sep="/")
  reviews_data <- read_csv(file_path)
  reviews_data$file_name <- reviews_list[i]
  
  normalise_reviews(reviews_data, included_listing)

}

# ----- Run ETL to normalise and combine calendar.csv.gz
calendar_tracker <- data.frame(listing_id=character(), date=as.Date(character())) # create empty df for function normalise_calendar
for (i in 1:length(calendars_list)) {
  
  file_path <- calendars_list[i]
  calendars_data <- read_csv(file_path)
  calendars_data$file_name <- calendars_list[i]
  
  normalise_calendar(calendars_data, included_listing)
}

dbListTables(conn) # list all table names

dbDisconnect(conn)
end_time <- Sys.time()
end_time - start_time #record how long it takes

```




