rm(list = ls()) # cleans the memory
library(xml2)
library(rvest)
library(stringr)
library(dplyr)
library(tm)
library(qdap) # Quantitative Discourse Analysis Package
library(ggplot2)
library(tidyr)
scrape_amazon <- function(ASIN, page_num){
url_reviews <- paste0("https://www.amazon.co.uk/product-reviews/",ASIN,"/?pageNumber=",page_num)
doc <- read_html(url_reviews) # Assign results to `doc`
# Review Title
doc %>%
html_nodes("[class='a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold']") %>%
html_text() -> review_title
# Review date
doc %>%
html_nodes("[data-hook='review-date']") %>%
html_text() -> review_date
# Review Text
doc %>%
html_nodes("[class='a-size-base review-text review-text-content']") %>%
html_text() -> review_text
# Number of stars in review
doc %>%
html_nodes("[data-hook='review-star-rating']") %>%
html_text() -> review_star
# Return a tibble
tibble(ASIN = ASIN,
review_title,
review_date,
review_text,
review_star,
page = page_num) %>% return()
}
clean.corpus <- function(corpus){
custom.stopwords <- c(stopwords('english'), 'lol', 'smh', 'delta', '\n')
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, custom.stopwords)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
#  corpus <- tm_map(corpus, stemCompletion)
#  corpus <- tm_map(corpus, removeNumbers)
return(corpus)}
# _____ automate how many pages to loop based on total number of reviews/10
# _____ get list of ASIN
ASIN_input <- "0007477155"
main_df <- data.frame()
# --- Get reviews from multiple page
for (k in 1:2){
local_df <- scrape_amazon(ASIN = ASIN_input, page_num = k)
main_df <- dplyr::bind_rows(main_df,local_df)
}
head(main_df)
trimws(main_df$review_text)
# _____ stemming make the words weird e.g. characters = charat, series = seri
# Make corpus
corpus_review <- Corpus(VectorSource(main_df$review_text))
corpus_review_cleaned <- clean.corpus(corpus_review)
# View corpus
corpus_review[[1]][1]
x <- corpus_review_cleaned[[1]][1]$content
qdap::polarity(x)
# Find the 20 most frequent terms: term_count
term_count <- freq_terms(corpus_review_cleaned, 20)
# Plot 20 most frequent terms
plot(term_count)
head(main_df)
corpus_review_cleaned
corpus_review[[1]][1]
x <- corpus_review_cleaned[[1]][1]$content
# View corpus
corpus_review[[1]][1]
# View corpus
corpus_review[[1]][1]
corpus_review_cleaned[[1]][1]
rm(list = ls()) # cleans the memory
library(xml2)
library(rvest)
library(stringr)
library(dplyr)
library(tm)
library(qdap) # Quantitative Discourse Analysis Package
library(ggplot2)
library(tidyr)
library(RCurl) # download files
library(readr) # read from zip
library(stringr)
library(RSQLite)
rm(list = ls()) # cleans the memory
library(xml2)
library(rvest)
library(stringr)
library(dplyr)
library(tm)
library(qdap) # Quantitative Discourse Analysis Package
library(ggplot2)
library(tidyr)
library(RCurl) # download files
library(readr) # read from zip
library(stringr)
library(RSQLite)
split_path <- function(path) {
rev(setdiff(strsplit(path,"/|\\\\")[[1]], ""))
}
# ----- Read HTML of the airbnb open data
url_airbnb <- 'http://insideairbnb.com/get-the-data.html'
doc <- read_html(url_airbnb) # Assign results to `doc`
web_list  <- doc %>%
html_nodes('tbody tr td') %>%
html_text()
# ----- Create a df in a tidy format on all datasets available including the archived ones
file_path  <- doc %>%  html_nodes('tbody tr td a') %>% html_attr('href') # file_path column
date_compiled  <- web_list[seq(1, length(web_list), 4)] # date_compiled column
country_or_city <- web_list[seq(2, length(web_list), 4)] # country_or_city column
file_name <- web_list[seq(3, length(web_list), 4)] # file_name column
description <- web_list[seq(4, length(web_list), 4)] # description column
date <- c() # get date column from file path
for (i in 1:length(file_path)) {
new_date <-split_path(file_path[i])[3]
date <- c(date, new_date)
}
airbnb_map_all <- data.frame(date_compiled, date, country_or_city, file_name, file_path, description, stringsAsFactors = FALSE) # airbnb_resource dataset
airbnb_map_all$date <- as.Date(airbnb_map_all$date)
#airbnb_data_map$date <- format(airbnb_data_map$date,"%d/%m/%y")
# ----- Filter only the latest datasets for each country and take only the zipped files
city_newest_update <- airbnb_map_all %>%
group_by(country_or_city) %>%
summarise(date = max(date))
to_download <- merge(airbnb_map_all, city_newest_update, by = c("country_or_city","date")) %>%
filter(file_name %in% c("reviews.csv.gz", "calendar.csv.gz", "listings.csv.gz"))
airbnb_map_all
to_download
to_download <- merge(airbnb_map_all, city_newest_update, by = c("country_or_city","date")) %>%
filter(file_name %in% c("reviews.csv.gz", "listings.csv.gz"))
to_download
merge_files  <- function(path, pattern) {
main_df <- data.frame()
list_files <- list.files(path, pattern = pattern)
for (i in length(list_files)) {
file_path <-paste(path,list_files[i],sep="/")
local_df <- read_csv(file_path)
local_df$file_name <- list_files[i]
main_df <- rbind(local_df,main_df)
}
return(main_df)
}
