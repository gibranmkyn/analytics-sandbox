---
title: "Airbnb Webscrapper"
author: "Gibran Makyanie"
date: "11/02/2020"
output: html_document
---

```{r setup, include=FALSE}
rm(list = ls()) # cleans the memory
library(xml2)
library(rvest)
library(dplyr)
library(qdap) # Quantitative Discourse Analysis Package
library(ggplot2)
library(RCurl) # download files
library(readr) # read from zip
library(stringr)
library(RSQLite)
# library(plyr)
library(cld3) # language detector
```


```{r}
split_path <- function(path) {
    rev(setdiff(strsplit(path,"/|\\\\")[[1]], ""))
} 
```

## Web Scrapping
The main goal of this section is to download the relevant datasets from the insideairbnb webpage. First, the URLs of the datasets along with its descriptions such as compiled date and city name are scrapped from the web using the xml2 package and compiled into a dataframe. Next, the dataframe is filtered to those relevant for the assignment stored in 'to_download' dataframe. Finally, a list of the relevant URLs are passed to a loop that downloads the data into a folder called 'dataset'.

```{r get the URLs of airbnb datasets}
# ----- Read HTML of the airbnb open data
url_airbnb <- 'http://insideairbnb.com/get-the-data.html'
doc <- read_html(url_airbnb) # Assign results to `doc`  

web_list  <- doc %>% 
          html_nodes('tbody tr td') %>%
          html_text() 

# ----- Create a df in a tidy format on all datasets available including the archived ones
file_path  <- doc %>%  html_nodes('tbody tr td a') %>% html_attr('href') # file_path column
date_compiled  <- web_list[seq(1, length(web_list), 4)] # date_compiled column
country_or_city <- web_list[seq(2, length(web_list), 4)] # country_or_city column
file_name <- web_list[seq(3, length(web_list), 4)] # file_name column
description <- web_list[seq(4, length(web_list), 4)] # description column
date <- c() # get date column from file path
    for (i in 1:length(file_path)) {
    new_date <-split_path(file_path[i])[3]
    date <- c(date, new_date)
    }

airbnb_map_all <- data.frame(date_compiled, date, country_or_city, file_name, file_path, description, stringsAsFactors = FALSE) # airbnb_resource dataset
airbnb_map_all$date <- as.Date(airbnb_map_all$date)
```


```{r filter the datasets to those relevant for the assignment}
# ----- Filter only the latest datasets for each country and take only the zipped files
city_newest_update <- airbnb_map_all %>%
   group_by(country_or_city) %>%
   summarise(date = max(date))

to_download <- merge(airbnb_map_all, city_newest_update, by = c("country_or_city","date")) %>%
  filter(file_name %in% c("reviews.csv.gz", "listings.csv.gz"))
```


```{r download relevant files}
dirname <- "dataset" 
dir.create(dirname) # create a new directory for the dataset

for (i in 1:length(to_download$file_path)) {
  tryCatch(download.file(to_download$file_path[i], destfile = paste0("dataset/",to_download$country_or_city[i], "_", to_download$file_name[i])), error = function(e) print('file didnot work' ))
}
```

## Extracting, Transforming, and Loading data to a relational schema

A complete ETL workflow has to be defined to get the relevant datasets into a normalised relational schema. This might be the most essential process for the whole assignment as for the famous phrase of data scieince, "garbage in, garbage out", we don't want that!

The ETL workflows are split into 3 major chunks; investigating the datasets, building a taylored fit ETL workflow, and running the workflow. First, samples of listings and reviews data are taken from each country to know which columns to keep and which ones to remove. It was found that the listings dataset can be normalised into reviewer and review, while listing can normalised into host and listing. Additionally, columns that have URLs are not needed, thus will be considered in the data transformation. 

> A reviewer writes a review for a listing, which is owned by a host.

Afterwards, the ETL work flow is built and immedietly used to the sample data as it has all the columns from every country to avoid errors later since relational schema has to be predefined before data are insereted. Ofcourse the tables are truncated to ensure clean sheet before loading the datasets. 

Finally, the datasets are loaded by running the ETL workflow.

```{r ETL investigation, message=FALSE}
# ----- Pre-ETL investigation to get all columns and select the columns needed

# Create a function to make a sample of 1 row from each dataset and combine them to get an idea what columns we have
get_sample <- function(folder_path, pattern) {

  listed_files <- list.files(folder_path, pattern = pattern)
  main_df <- data.frame()

  for (i in 1:length(listed_files)) {
    
    file_path <-paste(folder_path, listed_files[i],sep="/")
    local_df <- read.csv(file_path, nrows= 1, stringsAsFactors=FALSE)
    #local_df <- read_csv(file_path, n_max = 1)
    local_df$file_name <-listed_files[i]
    main_df <- plyr::rbind.fill(main_df, local_df)
    
  }

return(main_df)

}

# Generate the sample
listings_sample <- get_sample(folder_path = "dataset", pattern = "listings.csv.gz")
reviews_sample <- get_sample(folder_path = "dataset", pattern = "reviews.csv.gz")
reviews_sample$date <- as.Date(reviews_sample$date)

```


```{r ETL build workflow}

# ----- Initiation
conn <- dbConnect(RSQLite::SQLite(), "inside_airbnb.db") # connect to SQLite db, in this case, it created a new db


# ----- Build ETL workflow for listings data and reviews data

normalise_listings <- function(listings_data) {

  hosts_table <- listings_data %>%
    select(starts_with('host'), -contains("url")) %>% 
    distinct(host_id, .keep_all = TRUE)

  listings_table <- listings_data %>%
    select(-c(contains("url"), host_name:host_identity_verified, scrape_id, last_scraped, space, host_listings_count, smart_location))

  dbWriteTable(conn,"host", hosts_table, append = TRUE)
  dbWriteTable(conn,"listing", listings_table, append = TRUE)
}

normalise_reviews <- function(reviews_data) {
  
  reviewers_table <- reviews_data %>%
    distinct(reviewer_id, reviewer_name)
  
  reviews_table <- reviews_data %>% 
    select(-reviewer_name)
  
  dbWriteTable(conn,"review", reviews_table, append = TRUE)
  dbWriteTable(conn,"reviewer", reviewers_table, append = TRUE)
}


# ----- Automatically use sample data to create schema
normalise_reviews(reviews_sample)
normalise_listings(listings_sample)

# ----- Clear tables, ready to be inserted.
dbGetQuery(conn, "DELETE FROM review")
dbGetQuery(conn, "DELETE FROM reviewer")
dbGetQuery(conn, "DELETE FROM listing")
dbGetQuery(conn, "DELETE FROM host")
```


```{r ETL run workflow}
start_time <- Sys.time()

folder_path <- "dataset"
listings_list <-list.files(folder_path, pattern = "listings.csv.gz")
reviews_list <-list.files(folder_path, pattern = "reviews.csv.gz")

# ----- Run ETL to normalise listings.csv.gz and store in a relational schema
for (i in 1:length(listings_list)) {
  
  file_path <-paste(folder_path, listings_list[i],sep="/")
  listings_data <- read_csv(file_path)
  listings_data$file_name <-listings_list[i]

  normalise_listings(listings_data) # call function built especially to normalise listings

}

# ----- Run ETL to normalise reviews.csv.gz and store in a relational schema
for (i in 1:length(reviews_list)) {

  file_path <-paste(folder_path, reviews_list[i],sep="/")
  reviews_data <- read_csv(file_path)
  reviews_data$file_name <- reviews_list[i]
  
  normalise_reviews(reviews_data)

}

dbListTables(conn) # list all table names

dbDisconnect(conn)
end_time <- Sys.time()
end_time - start_time #record how long it takes

```




```{r}
conn <- dbConnect(RSQLite::SQLite(), "inside_airbnb.db") 

review_sample <- dbGetQuery(conn,'SELECT * FROM review limit 10000')
listing_sample <- dbGetQuery(conn,'SELECT * FROM listing limit 10000')



# ----- Remove non-english words
review_sample$lang <- detect_language(review_sample$comments)

summarised_x <- review_sample %>%
  group_by(nchar) %>%
  summarise(count_nchar = n())


review_sample %>%
  filter(nchar < 12)


%>%
  mutate(nchar = nchar(comments), word_count = lengths(gregexpr("\\W+", comments))) %>%
  filter(lang == "en", nchar == 1) %>%
  
# ----- 

ggplot(summarised_x, aes(x=count_nchar)) +
    geom_histogram(binwidth=.5, alpha=.5, position="identity")



```




